# -*- coding: utf-8 -*-
"""Cats Vs Dog.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1boAeA4ZFk-w2A7OoDy-UZqle3X1Gc1SX
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

import kagglehub

# Download latest version
path = kagglehub.dataset_download("salader/dogsvscats")

print("Path to dataset files:", path)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import os
import shutil

# Original Kaggle dataset path
kaggle_dataset_path = '/kaggle/input/dogsvscats'

# Colab cache directory
colab_cache_path = '/root/.cache/dogsvscats'

# Check if cache already exists
if not os.path.exists(colab_cache_path):
    print("Copying dataset to Colab cache...")
    shutil.copytree(kaggle_dataset_path, colab_cache_path)
else:
    print("Using cached dataset.")

# Set the dataset path for training or preprocessing
dataset_path = colab_cache_path
print("Dataset path:", dataset_path)

import tensorflow as tf

# Define the path to the training images
train_dir = '/root/.cache/dogsvscats/catsvsdogs/train'

# Load the images using image_dataset_from_directory
train_dataset = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    labels='inferred',
    label_mode='binary',
    image_size=(256,256),
    interpolation='nearest',
    batch_size=32,
    shuffle=True
)

# Print the type of the dataset
print(type(train_dataset))

# Print the class names
print(train_dataset.class_names)

test_dataset = tf.keras.utils.image_dataset_from_directory(
    directory='/root/.cache/dogsvscats/catsvsdogs/test',
    labels='inferred',
    label_mode='binary',
    batch_size=32,
    image_size=(256,256),
    shuffle=True,
    interpolation='nearest'
)

# Normalise the numpy array
def process(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

train_dataset = train_dataset.map(process)
test_dataset = test_dataset.map(process)

train_dataset

import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.layers import BatchNormalization

from tensorflow import keras

# create CNN Model
model = Sequential()

model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3),kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu',kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu',kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Flatten())

model.add(Dense(128,activation='relu',kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4)))
model.add(BatchNormalization())
model.add(Dropout(0.1))
model.add(Dense(64,activation='relu',kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4)))
model.add(BatchNormalization())
model.add(Dropout(0.1))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')

model.summary()

history = model.fit(train_dataset,epochs=10,validation_data=test_dataset)

import pickle
pickle.dump(model,open('model.pkl','wb'))

model.evaluate(test_dataset)

model.fit(train_dataset,epochs=10,validation_data=test_dataset)

pickle.dump(model,open('model.pkl','wb'))

model.save('Cat_dog.keras')

